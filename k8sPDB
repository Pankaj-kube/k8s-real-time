Use Case

Kubernetes Pod Disruption Budget Practical Guide

Have you ever faced a situation where your application suddenly went down during a cluster upgrade or scaling event? Frustrating, right?

Losing pods at the wrong moment can mean downtime, unhappy users, and stress all around. Well, here’s where Pod Disruption Budgets (PDB) step in to save the day.

What Exactly is a Pod Disruption Budget (PDB)?

A PDB makes sure that during voluntary disruptions (like a node drain) or involuntary events (like a node crash), a certain number of pods remain running. Without it, Kubernetes could easily kill off too many pods at once, leaving your service unavailable.

What Happens Without a PDB?

Imagine this: you’ve got 3 nodes, and your app is running 2 pods—App 1 on Node 1 and App 2 on Node 2. When Node 1 gets drained, App 1 is terminated, but App 3 is still pending and hasn’t started on Node 3 yet.

Now, if Node 2 gets drained while App 3 is still pending, App 2 is terminated as well. What are you left with? No healthy pods. Your service is down until the new pod is up.

What Happens With a PDB?

If you configure a PDB requiring that at least 1 pod must be running, this changes everything. When Node 1 is drained and App 1 terminates, App 3 starts getting ready. But if the system tries to drain Node 2 before App 3 is running, Kubernetes will block the drain to keep App 2 alive. This way, you’ll never be left without healthy pods, ensuring continuous service.
Why  Static POD needed 

###############################################################################
Static Pods are Pods that are created and managed directly by the kubelet daemon running on a specific node, rather than by the Kubernetes API server.
1. Running legacy applications: If you have legacy applications that cannot be easily containerized, you can use static Pods to run them on Kubernetes nodes.
2. Running essential system components: Static Pods can be used to run essential system components that need to be available at all times, even if the Kubernetes API server is unavailable. This can include network components, logging agents, and monitoring tools.

###########################################################################
Liveness and Readiness
Both liveness & readiness probes are used to control the health of an application.

- Failing liveness probe will restart the container 
- whereas failing readiness probe will stop our application from serving traffic.

side car run along with main container

####################################################################################
Quality of Service classes
Kubernetes classifies the Pods that you run and allocates each Pod into a specific quality of service (QoS) class. Kubernetes uses that classification to influence how different pods are handled. Kubernetes does this classification based on the resource requests of the Containers in that Pod, along with how those requests relate to resource limits. This is known as Quality of Service (QoS) class.
Guaranteed -Every container in the pod has memory and CPU requests equal to their limits.
Burstable - It has memory or CPU requests set, but limits are higher or not all containers have equal requests and limits.
BestEffort - No memory or CPU requests or limits are set for any container.
##################################################################################
Node Goes NotReady Due to Clock Skew
We are running tanzu cluster where workernodes created automatically.
We have checked NTP or chrony is not running /install on any node
but for few worker node on the pods we are  seeing clock Skew error
later we found vmware tool is not running on those vms after restart vmware tool on vms issue got resolved 
##################################################################################

ClusterIP Service
• Default Service Type: When you create a service without specifying a type, it defaults to ClusterIP.
• Single IP Address: It assigns a single IP address (ClusterIP) to a group of pods, which acts as a load balancer for distributing traffic among the pods.
• Internal Access: This IP address is only accessible within the Kubernetes cluster, making it suitable for internal communication between services1.
Headless Service
• No ClusterIP: Unlike ClusterIP services, headless services do not allocate a ClusterIP. Instead, they return the IP addresses of the individual pods directly.
• Direct Pod Access: This allows clients to connect directly to the pods without going through a load balancer. It is useful for stateful applications where you need to connect to specific pod instances.
• DNS Records: When a DNS query is made for a headless service, it returns the IP addresses of all the pods associated with the service, enabling direct communication with each pod2

NodePort
Exposes the Service on each Node's IP at a static port (the NodePort). To make the node port available, Kubernetes sets up a cluster IP address, the same as if you had requested a Service of type: ClusterIP. 
LoadBalancer
Exposes the Service externally using an external load balancer. Kubernetes does not directly offer a load balancing component; you must provide one, or you can integrate your Kubernetes cluster with a cloud provider.
ExternalName
Maps the Service to the contents of the externalName field (for example, to the hostname api.foo.bar.example). The mapping configures your cluster's DNS server to return a CNAME record with that external hostname value. No proxying of any kind is set up.
The type field in the Service API is designed as nested functionality - each l
######################################################################################

Finalizers are namespaced keys that tell Kubernetes to wait until specific conditions are met before it fully deletes resources marked for deletion. Finalizers alert controllers to clean up resources the deleted object owned.
Kubernetes keeps monitoring the Finalizers attached to the object. - The object will be deleted once the metadata.finalizers field is empty, because all Finalizers were removed by the completion of their actions.
kubectl patch pod example-pod -p '{"metadata: {"finalizers": null}}'
Stale Finalizers Preventing Namespace Deletion
kubectl patch ns <name> -p '{"spec":{"finalizers":[]}}' --type=merge
=======================================================================================
• Cilium is a networking, observability, and security solution with an eBPF-based data plane. Cilium provides a simple flat Layer 3 network with the ability to span multiple clusters in either a native routing or overlay/encapsulation mode, and can enforce network policies on L3-L7 using an identity-based security model that is decoupled from network addressing. Cilium can act as a replacement for kube-proxy; it also offers additional, opt-in observability and security features. Cilium is a CNCF project at the Graduated level.
• Multus is a Multi plugin for multiple network support in Kubernetes to support all CNI plugins (e.g. Calico, Cilium, Contiv, Flannel), in addition to SRIOV, DPDK, OVS-DPDK and VPP based workloads in Kubernetes.
• OVN-Kubernetes is a networking provider for Kubernetes based on OVN (Open Virtual Network), a virtual networking implementation that came out of the Open vSwitch (OVS) project. OVN-Kubernetes provides an overlay based networking implementation for Kubernetes, including an OVS based implementation of load balancing and network policy.
The operating system (OS) is a great place to add features like monitoring (observability), security, and networking because the core part of the OS, called the kernel, has special powers to watch over and manage the whole system. However, because the kernel is so important and needs to be very stable and secure, it’s hard to make changes to it. This means that new features and improvements happen more slowly in the OS compared to other parts of the system where changes are easier to make
eBPF (extended Berkeley Packet Filter) fundamentally changes how we can enhance the operating system. It allows small, secure programs to run directly within the OS. This means developers can add new features to the OS while it’s running, without needing to change the core system. The OS ensures these eBPF programs run safely and efficiently, thanks to tools like a Just-In-Time (JIT) compiler and a verification engine.
With eBPF, there’s now a new way to change how the Linux kernel behaves without needing to modify the kernel’s source code or load a kernel module. This is similar to how JavaScript and other scripting languages made it easier and cheaper to update systems that had become hard to change. eBPF allows developers to add new features and functionalities to the kernel dynamically, making the system more flexible and adaptable
###############################################################################################################


